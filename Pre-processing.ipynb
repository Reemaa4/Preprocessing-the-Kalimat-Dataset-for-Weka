{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Path to the main dataset folder\n",
    "\n",
    "# DATASET_DIR = 'dataset/'\n",
    "DATASET_DIR = '\\examples'\n",
    "\n",
    "\n",
    "# OUTPUT_DIR = 'output/'\n",
    "OUTPUT_DIR = '\\outputexamples'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Custom list of Arabic stopwords\n",
    "arabic_stopwords = [\n",
    "    'و', 'في', 'من', 'علي', 'عن', 'ما', 'هذا', 'تلك', 'كما', \n",
    "    'هي', 'هو', 'انا', 'نحن', 'ان', 'اذا', 'او', 'لكن', 'ثم', 'بل', 'الذي', \n",
    "    'التي', 'الذين', 'كل', 'اي', 'بعد', 'قبل', 'كان', 'قد', 'ليس', 'مع', \n",
    "    'عند', 'هنا', 'هناك', 'اذ', 'الي', 'ب', 'لم', 'لن', 'لا', 'ان', 'انه', \n",
    "    'ايضا', 'هذه', 'هولا', 'انت', 'هم', 'كن', 'انتم', 'اين', 'حين', \n",
    "    'كيف', 'كلما', 'بين', 'ام', 'ما', 'انما', 'بعض', 'مثل', \n",
    "    'نفس', 'بلا', 'دون', 'حتي', 'اما', 'عبر', 'كلا', 'امام', 'حيث', \n",
    "    'اذما', 'سوف', 'عندما', 'كلما', 'الذي', 'اللذين', 'اللتين', 'اللتان', \n",
    "    'الاولي', 'ابدا', 'اثنا', 'الان', 'اقل', 'بسبب', 'بما', 'بماذا', \n",
    "    'تحت', 'حسب', 'خلال', 'اكثر', 'اضافه', 'عليها', 'فيه', 'اليهم', \n",
    "    'علينا', 'ذلك', 'الذي', 'لقد', 'والتي','والذي','وهو','وهي',\n",
    "    'والذين','ومع','وحين','وحيث','ولقد','وذلك','وبسبب','وفيه','وماذا',\n",
    "    'وبعض','وتحت','وهنا','ومن','وان','وامام','وكلا','وعلي','واين','وكما',\n",
    "    'ولكن','وانما','والان','واثنا','وعند','وسوف','وبما','وتلك','وهذا',\n",
    "    'وما','وعن','وفي','وبل','وليس','واذا','ونحن','وانا','وقد','وكان',\n",
    "    'وقبل','وبعد','وكل','وانه','ولا','ولن','ولم','والي','واذ','وهناك'\n",
    "    ,'واين','وانتم','وكن','وهم','وانت','انتي','وانتي','وهولا'\n",
    "]\n",
    "\n",
    "# Define prefixes and suffixes\n",
    "larkey_defarticles = (u\"ال\", u\"وال\", u\"بال\", u\"كال\", u\"فال\", u\"لل\")\n",
    "larkey_suffixes = (u\"ها\", u\"ان\", u\"ات\", u\"ون\", u\"ين\", u\"يه\", u\"ه\")\n",
    "\n",
    "# Function to normalize Arabic text\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(r'[\\u064B-\\u0652]', '', text)  # Remove diacritics\n",
    "    text = re.sub(r'[إأآا]', 'ا', text)  # Normalize Alef\n",
    "    text = re.sub(r'ة', 'ه', text)  # Normalize Ta Marbuta\n",
    "    text = re.sub(r'ى', 'ي', text)  # Normalize Ya\n",
    "    text = re.sub(r'ء', '', text)  # Remove Hamza\n",
    "    text = re.sub(r'ؤ', 'و', text)  # Remove Hamza\n",
    "    text = re.sub(r'[0-9٠-٩]', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation \n",
    "    return text\n",
    "\n",
    "# Custom Arabic sentence tokenizer based on punctuation\n",
    "def custom_arabic_sent_tokenize(text):\n",
    "    sentences = re.split(r'[.!؟،:,]\\s*', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def remove_stopwords_and_affixes(sentence, stopwords, prefixes, suffixes):\n",
    "    words = sentence.split()\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Only add the word if it’s not a stopword\n",
    "        if word not in stopwords:\n",
    "            # Remove prefixes if they exist\n",
    "            for prefix in prefixes:\n",
    "                if word.startswith(prefix):\n",
    "                    word = word[len(prefix):]  # Remove the prefix\n",
    "                    break  # Stop after removing the first matching prefix\n",
    "\n",
    "            # Remove suffixes if they exist\n",
    "            for suffix in suffixes:\n",
    "                if word.endswith(suffix):\n",
    "                    word = word[:-len(suffix)]  # Remove the suffix\n",
    "                    break  # Stop after removing the first matching suffix\n",
    "\n",
    "            # Add the processed word if it’s not empty after removing prefixes/suffixes\n",
    "            if word.strip():\n",
    "                filtered_words.append(word)\n",
    "\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "# Function to process text files and extract features\n",
    "def process_files():\n",
    "    all_sentences = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for label_folder in os.listdir(DATASET_DIR):\n",
    "        label_folder_path = os.path.join(DATASET_DIR, label_folder)\n",
    "        \n",
    "        if os.path.isdir(label_folder_path):\n",
    "            output_label_folder = os.path.join(OUTPUT_DIR, label_folder)\n",
    "            os.makedirs(output_label_folder, exist_ok=True)\n",
    "            \n",
    "            for txt_file in os.listdir(label_folder_path):\n",
    "                if txt_file.endswith('.txt'):\n",
    "                    txt_file_path = os.path.join(label_folder_path, txt_file)\n",
    "                    \n",
    "                    with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "                        text = f.read()\n",
    "                        normalized_text = normalize_arabic(text)\n",
    "                        tokenized_sentences = custom_arabic_sent_tokenize(normalized_text)\n",
    "                        \n",
    "                        cleaned_sentences = [remove_stopwords_and_affixes(sentence, arabic_stopwords, larkey_defarticles, larkey_suffixes) for sentence in tokenized_sentences]\n",
    "                        \n",
    "                        if any(cleaned_sentences):\n",
    "                            all_sentences.append(' '.join(cleaned_sentences))\n",
    "                            all_labels.append(label_folder)\n",
    "                            \n",
    "                        output_file_path = os.path.join(output_label_folder, f'processed_{txt_file}')\n",
    "                        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                            output_file.write('\\n'.join(cleaned_sentences))\n",
    "\n",
    "    return all_sentences, all_labels\n",
    "\n",
    "\n",
    "all_sentences, all_labels = process_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been saved to C:\\Users\\Lapto\\OneDrive\\سطح المكتب\\NLP assignment\\example_articles.arff\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def text_to_arff(output_file, dataset_folder):\n",
    "    # Open the output ARFF file\n",
    "    with open(output_file, 'w', encoding='utf-8') as arff_file:\n",
    "        # Write the ARFF header\n",
    "        arff_file.write(\"@relation arabic_text_classification\\n\\n\")\n",
    "        arff_file.write(\"@attribute text string\\n\")\n",
    "        arff_file.write(\"@attribute class {Culture, Economy, Local, International, Religion, Sports}\\n\\n\")\n",
    "        arff_file.write(\"@data\\n\")\n",
    "        \n",
    "        # Iterate over the categories (sub-folders)\n",
    "        for category in os.listdir(dataset_folder):\n",
    "            category_folder = os.path.join(dataset_folder, category)\n",
    "            if os.path.isdir(category_folder):\n",
    "                # Iterate over all files in the category\n",
    "                for filename in os.listdir(category_folder):\n",
    "                    file_path = os.path.join(category_folder, filename)\n",
    "                    if filename.endswith('.txt'):\n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                            text = file.read().replace('\\n', ' ').replace('\\r', '')\n",
    "                            # Write the data to the ARFF file\n",
    "                            arff_file.write(f\"'{text}',{category}\\n\")\n",
    "\n",
    "# Path to your dataset and output file\n",
    "# dataset_folder = r'C:\\Users\\Lapto\\OneDrive\\سطح المكتب\\NLP assignment\\output'\n",
    "dataset_folder = r'C:\\Users\\Lapto\\OneDrive\\سطح المكتب\\NLP assignment\\outputexamples'\n",
    "\n",
    "# output_file = r'C:\\Users\\Lapto\\OneDrive\\سطح المكتب\\NLP assignment\\ar_text_dataset.arff'\n",
    "output_file = r'C:\\Users\\Lapto\\OneDrive\\سطح المكتب\\NLP assignment\\example_articles.arff'\n",
    "\n",
    "# Convert the dataset to ARFF format\n",
    "text_to_arff(output_file, dataset_folder)\n",
    "\n",
    "print(f\"Dataset has been saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
